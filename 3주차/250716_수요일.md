# 2025-07-16 수요일

## Streamlit 실습


### 주식 차트 실습
```python
# https://hello-yeonji-stock.streamlit.app/

# 표준 라이브러리
import datetime
from io import BytesIO

# 서드파티 라이브러리
import streamlit as st
import pandas as pd
import FinanceDataReader as fdr
import plotly.graph_objects as go
import matplotlib.pyplot as plt
import matplotlib
import koreanize_matplotlib

# 캐싱: 인자가 바뀌지 않는 함수 실행 결과를 저장 후 재사용
@st.cache_data
def get_krx_company_list() -> pd.DataFrame:
    """
    KRX(한국거래소) 상장 기업의 회사명과 종목코드 정보를 DataFrame으로 반환합니다.

    Returns:
        pd.DataFrame: '회사명', '종목코드' 컬럼을 가진 DataFrame
    """
    krx_df = fdr.StockListing('KRX')
    company_df = krx_df[['Name', 'Code']].rename(columns={'Name': '회사명', 'Code': '종목코드'})
    return company_df


def get_stock_code_by_company(company_name: str) -> str:
    """
    회사명을 입력받아 해당 회사의 종목코드를 반환합니다.
    Args:
        company_name (str): 조회할 회사명
    Returns:
        str: 종목코드. 입력된 회사명이 없으면 ValueError 발생
    """
    company_df = get_krx_company_list()
    codes = company_df[company_df['회사명'] == company_name]['종목코드'].values
    if len(codes) > 0:
        return codes[0]
    else:
        raise ValueError(f"'{company_name}'에 해당하는 종목코드가 없습니다.")


def sidebar_inputs() -> tuple[str, tuple[datetime.date, datetime.date], bool]:
    """
    Streamlit 사이드바에 회사명 입력창, 날짜 선택 위젯, 확인 버튼을 생성하고 입력값을 반환합니다.

    Returns:
        tuple: (회사명(str), (시작일, 종료일)(tuple of date), 확인버튼 클릭여부(bool))
    """
    company_name = st.sidebar.text_input('회사 이름을 입력하세요: ')
    today = datetime.datetime.now()
    this_year = today.year
    jan_1 = datetime.date(this_year, 1, 1)
    selected_dates = st.sidebar.date_input(
        "시작일과 종료일을 입력하세요",
        (jan_1, today),
        None,
        today,
        format="MM.DD.YYYY",
    )
    st.sidebar.write(selected_dates)
    confirm_btn = st.sidebar.button('확인')
    return company_name, selected_dates, confirm_btn

# 우리가 필요로하는 코드조각들
stock_code = get_stock_code_by_company(company_name)
start_date = selected_dates[0].strftime(r"%Y-%m-%d")
end_date = (selected_dates[1] + datetime.timedelta(days=1)).strftime(r"%Y-%m-%d")
price_df = fdr.DataReader(f'KRX:{stock_code}', start_date, end_date)

excel_data = BytesIO()
price_df.to_excel(excel_data)
st.download_button("엑셀 파일 다운로드", excel_data, file_name='stock_data.xlsx')

```


### 직접 만든 웹크롤러

```python
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import time
import csv
from datetime import datetime, timedelta

# Chrome WebDriver 자동 설치
try:
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service)
except Exception as e:
    print(f"WebDriver 설치 및 실행 오류: {e}")
    exit()

# 데이터를 누적할 리스트를 미리 선언
all_news_data = []

# 크롤링할 날짜 범위 설정 (오늘부터 30일 전까지)
end_date = datetime.now()
start_date = end_date - timedelta(days=30)

# 날짜별로 루프 실행
current_date = start_date
while current_date <= end_date:
    date_str = current_date.strftime('%Y%m%d')
    url = f"https://news.naver.com/breakingnews/section/101/258?date={date_str}"

    print(f"\n### 날짜: {date_str} 기사 크롤링 시작 ###")
    driver.get(url)
    time.sleep(2)

    # '기사 더보기' 버튼이 사라질 때까지 클릭하는 루프
    while True:
        try:
            more_button = driver.find_element(By.CLASS_NAME, 'section_more_inner')
            more_button.click()
            time.sleep(1)
        except Exception:
            print("더 이상 '더보기' 버튼이 없습니다. 다음 날짜로 넘어갑니다.")
            break

    # 모든 기사가 로딩된 후, BeautifulSoup으로 HTML 파싱
    soup = BeautifulSoup(driver.page_source, 'html.parser')
    news_items = soup.find_all('li', class_='sa_item')

    if not news_items:
        print("뉴스 기사를 찾을 수 없습니다.")
    else:
        for item in news_items:
            title_element = item.find('a', class_='sa_text_title')

            if title_element:
                try:
                    link = title_element['href']
                    title = title_element.find('strong').get_text(strip=True)
                    all_news_data.append({
                        '날짜': date_str,
                        '제목': title,
                        '링크': link
                    })
                except (KeyError, AttributeError):
                    continue

    # 다음 날짜로 이동
    current_date += timedelta(days=1)

# 모든 크롤링이 끝난 후 CSV 파일로 저장
file_name = 'naver_news_one_month.csv'
fields = ['날짜', '제목', '링크']

if all_news_data:
    with open(file_name, 'w', newline='', encoding='utf-8-sig') as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=fields)
        writer.writeheader()
        writer.writerows(all_news_data)
    print(f"\n총 {len(all_news_data)}개의 기사를 {file_name} 파일에 저장했습니다.")
else:
    print("저장할 기사가 없습니다.")

# 브라우저 닫기
driver.quit()
```

## 오늘의 포인트
- fisa05-stock.streamlit.app
- streamlit으로 간단하게 대시보드를 배포할 수 있다.

- 직접 네이버 증권기사 1달치 크롤링
- 원하는 정확한 태그 찾아내기는 수작업으로 하고 Gemini에 정확한 instruction 주니까 금방 완성함
- 생성형AI를 잘 사용하면 효율을 잘 뽑아낼 수 있다.